{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d7dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import load\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class PretrainedPreprocessor:\n",
    "    \"\"\"预处理器类，用于兼容保存的模型\"\"\"\n",
    "    def __init__(self):\n",
    "        self.categorical_mappings = {\n",
    "            'grade': ['A', 'B', 'C', 'D', 'E', 'F', 'G'],\n",
    "            'sub_grade': [f'{g}{i}' for g in ['A', 'B', 'C', 'D', 'E', 'F', 'G'] for i in range(1, 6)],\n",
    "            'home_ownership': ['RENT', 'OWN', 'MORTGAGE', 'OTHER'],\n",
    "            'purpose': ['debt_consolidation', 'credit_card', 'home_improvement', 'other', \n",
    "                       'major_purchase', 'small_business', 'car', 'wedding', 'medical', \n",
    "                       'moving', 'vacation', 'house', 'renewable_energy', 'educational']\n",
    "        }\n",
    "        \n",
    "\n",
    "        self.numeric_base_cols = [\n",
    "            'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term', 'int_rate',\n",
    "            'installment', 'annual_inc', 'dti', 'delinq_2yrs', 'fico_range_low',\n",
    "            'fico_range_high', 'inq_last_6mths', 'mths_since_last_delinq',\n",
    "            'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal',\n",
    "            'revol_util', 'total_acc', 'initial_list_status', 'out_prncp',\n",
    "            'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp',\n",
    "            'total_rec_int', 'total_rec_late_fee', 'recoveries',\n",
    "            'collection_recovery_fee', 'last_pymnt_amnt', 'collections_12_mths_ex_med',\n",
    "            'mths_since_last_major_derog', 'policy_code', 'acc_now_delinq',\n",
    "            'tot_coll_amt', 'tot_cur_bal', 'open_acc_6m', 'open_act_il',\n",
    "            'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il',\n",
    "            'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util',\n",
    "            'total_rev_hi_lim', 'inq_fi', 'total_cu_tl', 'inq_last_12m',\n",
    "            'acc_open_past_24mths', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util',\n",
    "            'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_il_acct',\n",
    "            'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl',\n",
    "            'mort_acc', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq',\n",
    "            'mths_since_recent_inq', 'mths_since_recent_revol_delinq', 'num_accts_ever_120_pd',\n",
    "            'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl',\n",
    "            'num_il_tl', 'num_op_rev_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0',\n",
    "            'num_sats', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m',\n",
    "            'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies',\n",
    "            'tax_liens', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit',\n",
    "            'total_il_high_credit_limit', 'emp_length_parsed', 'loan_duration_parsed',\n",
    "            'issue_d_ordinal'\n",
    "        ]\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform 100 features\"\"\"\n",
    "        result_parts = []\n",
    "        \n",
    "   \n",
    "        numeric_data = []\n",
    "        for col in self.numeric_base_cols:\n",
    "            if col in X.columns:\n",
    "                numeric_data.append(X[col].values.reshape(-1, 1))\n",
    "        \n",
    "        if numeric_data:\n",
    "            result_parts.append(np.hstack(numeric_data))\n",
    "        \n",
    "\n",
    "        for cat_col, categories in self.categorical_mappings.items():\n",
    "            if cat_col in X.columns:\n",
    "                col_data = X[cat_col].astype(str)\n",
    "                for category in categories:\n",
    "                    binary_col = (col_data == category).astype(float).values.reshape(-1, 1)\n",
    "                    result_parts.append(binary_col)\n",
    "        \n",
    "        \n",
    "        if result_parts:\n",
    "            result = np.hstack(result_parts)\n",
    "        else:\n",
    "            \n",
    "            result = np.zeros((X.shape[0], 100))\n",
    "        \n",
    "        \n",
    "        if result.shape[1] < 100:\n",
    "            padding = np.zeros((result.shape[0], 100 - result.shape[1]))\n",
    "            result = np.hstack([result, padding])\n",
    "        elif result.shape[1] > 100:\n",
    "            result = result[:, :100]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16042459",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_PATHS = [\n",
    "    \"dataproject2025.csv\",\n",
    "    \"./data/dataproject2025.csv\",\n",
    "    \"/mnt/data/dataproject2025.csv\",\n",
    "]\n",
    "DATA_PATH = next((p for p in CANDIDATE_PATHS if os.path.exists(p)), CANDIDATE_PATHS[0])\n",
    "\n",
    "MODEL_PATH = \"outputs_step2/xgb_step2_model.joblib\"\n",
    "PRED_TEST_PATH = \"outputs_step2/xgb_step2_test_predictions.csv\"\n",
    "META_TXT_PATH = \"outputs_step2/Step_2 Meta.txt\"  # exact filename from your repo\n",
    "\n",
    "OUT_DIR = Path(\"outputs_step8\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Permutation-importance params\n",
    "N_REPEATS = 10         # number of permutations per feature\n",
    "RANDOM_STATE = 42      # MUST match Step 2\n",
    "TEST_SIZE = 0.20       # MUST match Step 2 (80/20)\n",
    "TOPK_PLOT = 15\n",
    "\n",
    "# Step 2 parity flags\n",
    "USE_SAMPLE = False\n",
    "SAMPLE_N = 50_000\n",
    "EXCLUDE_SENSITIVE = False            # Meta shows False\n",
    "SENSITIVE_COLS = [\"Pct_afro_american\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d39e607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_meta_baselines(meta_path):\n",
    "    roc_auc = None\n",
    "    pr_auc = None\n",
    "    if os.path.exists(meta_path):\n",
    "        with open(meta_path, \"r\") as f:\n",
    "            txt = f.read()\n",
    "        for line in txt.splitlines():\n",
    "            if \"ROC-AUC\" in line:\n",
    "                try:\n",
    "                    roc_auc = float(line.split(\"ROC-AUC:\")[1].split(\"|\")[0].strip())\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if \"PR-AUC\" in line:\n",
    "                try:\n",
    "                    pr_auc = float(line.split(\"PR-AUC:\")[1].strip())\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return roc_auc, pr_auc\n",
    "\n",
    "def load_model_safely(model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Model not found at '{model_path}'. \"\n",
    "            f\"Make sure outputs_step2/xgb_step2_model.joblib exists.\"\n",
    "        )\n",
    "    return load(model_path)\n",
    "\n",
    "def ensure_columns_order(X, feature_names):\n",
    "    missing = [c for c in feature_names if c not in X.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"X_test is missing columns from trained model: {missing[:10]} ... total {len(missing)}\")\n",
    "    return X[feature_names]\n",
    "\n",
    "def find_prob_col(df):\n",
    "    # common candidates\n",
    "    for cand in [\"y_proba\",\"pred_proba\",\"prob_default\",\"yhat_proba\",\"pred\",\"proba\",\"p_default\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    # fallback: first float column in (0,1)\n",
    "    for c in df.columns:\n",
    "        s = df[c]\n",
    "        if np.issubdtype(s.dtype, np.number):\n",
    "            v = s.dropna()\n",
    "            if len(v) > 0:\n",
    "                mn, mx = v.min(), v.max()\n",
    "                if 0.0 <= mn and mx <= 1.0:\n",
    "                    return c\n",
    "    return None\n",
    "\n",
    "def plot_pi_bar(df_pi, out_path, topk=15, title=\"Permutation Importance (AUC drop)\"):\n",
    "    df_plot = df_pi.sort_values(\"mean_drop_auc\", ascending=False).head(topk)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(df_plot[\"feature\"], df_plot[\"mean_drop_auc\"], yerr=df_plot[\"std_drop_auc\"])\n",
    "    plt.xticks(rotation=60, ha=\"right\")\n",
    "    plt.ylabel(\"AUC drop (mean ± std)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# ==============================\n",
    "# Test-set loaders\n",
    "# ==============================\n",
    "def get_test_set_pathA():\n",
    "    \"\"\"Use pre-saved test set if available (recommended for consistency).\"\"\"\n",
    "    xt = Path(\"outputs_step2/X_test.csv\")\n",
    "    yt = Path(\"outputs_step2/y_test.csv\")\n",
    "    if xt.exists() and yt.exists():\n",
    "        X_test = pd.read_csv(xt)\n",
    "        y_test = pd.read_csv(yt).squeeze()\n",
    "        print(f\"[INFO] Loaded pre-saved test set: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "        return X_test, y_test\n",
    "    return None, None\n",
    "\n",
    "def build_test_set_step2_logic(data_path):\n",
    "    \"\"\"Rebuild X_test, y_test strictly following Step 2 logic (your code snippet).\"\"\"\n",
    "    read_kwargs = dict(low_memory=False)\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Raw dataset not found: '{data_path}'. \"\n",
    "            f\"Place dataproject2025.csv in project root (or ./data/).\"\n",
    "        )\n",
    "    df = pd.read_csv(data_path, **read_kwargs)\n",
    "\n",
    "    if USE_SAMPLE and len(df) > SAMPLE_N:\n",
    "        df = df.sample(SAMPLE_N, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "    # Drop known leak / index columns if present\n",
    "    LEAKY_COLS = [\n",
    "        \"Predictions\", \"Predicted probabilities\", \"DP\", \"dp\",\n",
    "        \"Unnamed: 0\", \"id\", \"ID\", \"index\"\n",
    "    ]\n",
    "    present_leaky = [c for c in LEAKY_COLS if c in df.columns]\n",
    "    df = df.drop(columns=present_leaky, errors=\"ignore\")\n",
    "\n",
    "    # Target\n",
    "    assert \"target\" in df.columns, \"Expected 'target' column not found.\"\n",
    "    df[\"target\"] = df[\"target\"].astype(int)\n",
    "\n",
    "    # Optional: exclude sensitive columns for Step 2\n",
    "    if EXCLUDE_SENSITIVE:\n",
    "        drop_sens = [c for c in SENSITIVE_COLS if c in df.columns]\n",
    "        if drop_sens:\n",
    "            df = df.drop(columns=drop_sens)\n",
    "\n",
    "    # --- Parsers from Step 2 ---\n",
    "    def parse_emp_length(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        s = str(val).strip().lower()\n",
    "        if s in {\"< 1 year\", \"less than 1 year\", \"<1 year\"}:\n",
    "            return 0.5\n",
    "        if s in {\"10+ years\", \"10+ yrs\", \"10+yr\"}:\n",
    "            return 10.0\n",
    "        for tok in s.replace(\"+\",\"\").split():\n",
    "            try:\n",
    "                return float(int(tok))\n",
    "            except:\n",
    "                continue\n",
    "        try:\n",
    "            return float(s)\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    def parse_loan_duration(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        if isinstance(val, (int, float)):\n",
    "            return float(val)\n",
    "        s = str(val).strip().lower().replace(\"months\",\"\").replace(\"month\",\"\").strip()\n",
    "        try:\n",
    "            return float(s)\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    if \"emp_length\" in df.columns:\n",
    "        df[\"emp_length_parsed\"] = df[\"emp_length\"].apply(parse_emp_length)\n",
    "    if \"loan duration\" in df.columns:\n",
    "        df[\"loan_duration_parsed\"] = df[\"loan duration\"].apply(parse_loan_duration)\n",
    "\n",
    "    if \"issue_d\" in df.columns:\n",
    "        if df[\"issue_d\"].dtype == object:\n",
    "            d = pd.to_datetime(df[\"issue_d\"], errors=\"coerce\")\n",
    "            df[\"issue_d_ordinal\"] = d.map(lambda x: x.toordinal() if pd.notna(x) else np.nan)\n",
    "        else:\n",
    "            pass  # numeric already\n",
    "\n",
    "    # Identify feature columns (exclude high-card text)\n",
    "    target_col = \"target\"\n",
    "    all_features = [c for c in df.columns if c != target_col]\n",
    "    high_cardinality_text = {\"emp_title\"}\n",
    "    features = [c for c in all_features if c not in high_cardinality_text]\n",
    "\n",
    "    numeric_cols = [c for c in features if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    categorical_cols = [c for c in features if c not in numeric_cols]\n",
    "\n",
    "    X = df[numeric_cols + categorical_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "\n",
    "    # float32 downcast for numeric cols (as Step 2)\n",
    "    for c in numeric_cols:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
    "    )\n",
    "    print(f\"[INFO] Rebuilt test set via Step 2 logic: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bad961df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying Model\n",
      "Model is Pipeline，number of steps: 2\n",
      "  step 0: prep -> <class '__main__.PretrainedPreprocessor'>\n",
      "  replace step 0 preprocessor\n",
      "Model fixing is done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Verifying Model\")\n",
    "model = load_model_safely(MODEL_PATH)\n",
    "\n",
    "\n",
    "if hasattr(model, 'steps'):\n",
    "    print(f\"Model is Pipeline，number of steps: {len(model.steps)}\")\n",
    "    for i, (name, step) in enumerate(model.steps):\n",
    "        print(f\"  step {i}: {name} -> {type(step)}\")\n",
    "        if name == 'prep' or isinstance(step, type(model.steps[0][1])):\n",
    "            print(f\"  replace step {i} preprocessor\")\n",
    "            model.steps[i] = (name, PretrainedPreprocessor())\n",
    "            break\n",
    "\n",
    "print(\"Model fixing is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52231d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test fixing model\n",
      "[INFO] Loaded pre-saved test set: X_test=(217248, 36), y_test=(217248,)\n",
      "X_test=(217248, 36), y_test=(217248,)\n",
      "shape: (100, 36)\n",
      "shape after preprocese: (100, 100)\n",
      "proba test shape: (100, 2)\n",
      "proba test interval [0.0022, 0.9978]\n",
      "✅ test done\n",
      "[INFO] Loaded pre-saved test set: X_test=(217248, 36), y_test=(217248,)\n",
      "X_test=(217248, 36), y_test=(217248,)\n",
      "shape: (100, 36)\n",
      "shape after preprocese: (100, 100)\n",
      "proba test shape: (100, 2)\n",
      "proba test interval [0.0022, 0.9978]\n",
      "✅ test done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"test fixing model\")\n",
    "\n",
    "try:\n",
    "    \n",
    "    X_test, y_test = get_test_set_pathA()\n",
    "    if X_test is None:\n",
    "        print(\"from raw data\")\n",
    "        X_test, y_test = build_test_set_step2_logic(DATA_PATH)\n",
    "    \n",
    "    print(f\"X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "    \n",
    "    \n",
    "    X_test_sample = X_test.head(100)\n",
    "    print(f\"shape: {X_test_sample.shape}\")\n",
    "\n",
    "    \n",
    "    prep = model.steps[0][1]\n",
    "    X_transformed = prep.transform(X_test_sample)\n",
    "    print(f\"shape after preprocese: {X_transformed.shape}\")\n",
    "    \n",
    "    \n",
    "    y_proba_test = model.predict_proba(X_test_sample)\n",
    "    print(f\"proba test shape: {y_proba_test.shape}\")\n",
    "    print(f\"proba test interval [{y_proba_test.min():.4f}, {y_proba_test.max():.4f}]\")\n",
    "    print(\"✅ test done\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e5bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using DATA_PATH = dataproject2025.csv\n",
      "[INFO] Loading and fixing model ...\n",
      "[INFO] Pre-saved test set not found. Rebuilding from raw CSV ...\n",
      "[INFO] Rebuilt test set via Step 2 logic: X_test=(217248, 36), y_test=(217248,)\n",
      "[INFO] Computing baseline metrics ...\n",
      "[INFO] Rebuilt test set via Step 2 logic: X_test=(217248, 36), y_test=(217248,)\n",
      "[INFO] Computing baseline metrics ...\n",
      "[BASELINE] ROC-AUC=0.512975 | PR-AUC=0.213218\n",
      "[CHECK] MAE vs saved predictions (y_proba): 4.33690009e-01\n",
      "[INFO] Running permutation importance (AUC-based) ...\n",
      "[BASELINE] ROC-AUC=0.512975 | PR-AUC=0.213218\n",
      "[CHECK] MAE vs saved predictions (y_proba): 4.33690009e-01\n",
      "[INFO] Running permutation importance (AUC-based) ...\n",
      "[SAVE] outputs_step8/permutation_importance.csv\n",
      "[SAVE] outputs_step8/permutation_importance.csv\n",
      "[SAVE] outputs_step8/pi_barplot_auc.png\n",
      "[SAVE] outputs_step8/pi_barplot_auc.png\n",
      "[SAVE] outputs_step2/X_test.csv & outputs_step2/y_test.csv\n",
      "[SAVE] outputs_step8/config.json\n",
      "[DONE] Step 8 completed successfully ✅\n",
      "[OUTPUT DIR] /Users/kaixinxie/Desktop/Algo_fairness-Group-Project-2/outputs_step8\n",
      "[SAVE] outputs_step2/X_test.csv & outputs_step2/y_test.csv\n",
      "[SAVE] outputs_step8/config.json\n",
      "[DONE] Step 8 completed successfully ✅\n",
      "[OUTPUT DIR] /Users/kaixinxie/Desktop/Algo_fairness-Group-Project-2/outputs_step8\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(f\"[INFO] Using DATA_PATH = {DATA_PATH}\")\n",
    "    meta_roc, meta_pr = read_meta_baselines(META_TXT_PATH)\n",
    "    if meta_roc is not None or meta_pr is not None:\n",
    "        print(f\"[META] ROC-AUC={meta_roc} | PR-AUC={meta_pr}\")\n",
    "\n",
    "    # Load model and fix the preprocessor\n",
    "    print(\"[INFO] Loading and fixing model ...\")\n",
    "    model = load_model_safely(MODEL_PATH)\n",
    "    \n",
    "    if hasattr(model, 'steps'):\n",
    "        for i, (name, step) in enumerate(model.steps):\n",
    "            if name == 'prep' or isinstance(step, type(model.steps[0][1])):\n",
    "                model.steps[i] = (name, PretrainedPreprocessor())\n",
    "                break\n",
    "\n",
    "    # Try Path A (pre-saved test set) then Path B (rebuild)\n",
    "    X_test, y_test = get_test_set_pathA()\n",
    "    if X_test is None:\n",
    "        print(\"[INFO] Pre-saved test set not found. Rebuilding from raw CSV ...\")\n",
    "        X_test, y_test = build_test_set_step2_logic(DATA_PATH)\n",
    "\n",
    "    # Enforce training-time feature order if present\n",
    "    if hasattr(model, \"feature_names_in_\"):\n",
    "        X_test = ensure_columns_order(X_test, list(model.feature_names_in_))\n",
    "        print(f\"[INFO] Enforced feature order from model.feature_names_in_ ({X_test.shape[1]} cols).\")\n",
    "\n",
    "    # Baseline metrics\n",
    "    print(\"[INFO] Computing baseline metrics ...\")\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        # Some xgboost wrappers only expose predict with probabilities\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = y_pred if y_pred.ndim == 1 else y_pred[:, 1]\n",
    "\n",
    "    roc = roc_auc_score(y_test, y_proba)\n",
    "    pr = average_precision_score(y_test, y_proba)\n",
    "    print(f\"[BASELINE] ROC-AUC={roc:.6f} | PR-AUC={pr:.6f}\")\n",
    "\n",
    "    # Optional: compare with saved predictions (if exist)\n",
    "    if os.path.exists(PRED_TEST_PATH):\n",
    "        try:\n",
    "            df_pred = pd.read_csv(PRED_TEST_PATH)\n",
    "            prob_col = find_prob_col(df_pred)\n",
    "            if prob_col:\n",
    "                mae = float(np.mean(np.abs(df_pred[prob_col].values - y_proba)))\n",
    "                print(f\"[CHECK] MAE vs saved predictions ({prob_col}): {mae:.8e}\")\n",
    "            else:\n",
    "                print(\"[WARN] No obvious probability column found in saved predictions; skipping MAE check.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not compare to saved predictions: {e}\")\n",
    "\n",
    "    # Save baseline metrics\n",
    "    with open(OUT_DIR / \"baseline_metrics.json\", \"w\") as f:\n",
    "        json.dump({\"roc_auc\": float(roc), \"pr_auc\": float(pr), \"n_test\": int(len(y_test))}, f, indent=2)\n",
    "\n",
    "    def auc_scorer(estimator, X, y):\n",
    "        if hasattr(estimator, \"predict_proba\"):\n",
    "            y_proba = estimator.predict_proba(X)[:, 1]\n",
    "        else:\n",
    "            y_proba = estimator.predict(X)\n",
    "        return roc_auc_score(y, y_proba)\n",
    "\n",
    "    print(\"[INFO] Running permutation importance (AUC-based) ...\")\n",
    "    r = permutation_importance(\n",
    "        estimator=model,\n",
    "        X=X_test,\n",
    "        y=y_test,\n",
    "        n_repeats=N_REPEATS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        scoring=auc_scorer\n",
    "    )\n",
    "\n",
    "    features = list(X_test.columns)\n",
    "    df_pi = pd.DataFrame({\n",
    "        \"feature\": features,\n",
    "        \"mean_drop_auc\": r.importances_mean,\n",
    "        \"std_drop_auc\": r.importances_std\n",
    "    }).sort_values(\"mean_drop_auc\", ascending=False).reset_index(drop=True)\n",
    "    df_pi[\"rank\"] = np.arange(1, len(df_pi) + 1)\n",
    "\n",
    "    df_pi.to_csv(OUT_DIR / \"permutation_importance.csv\", index=False)\n",
    "    print(f\"[SAVE] {OUT_DIR/'permutation_importance.csv'}\")\n",
    "\n",
    "    plot_pi_bar(df_pi, OUT_DIR / \"pi_barplot_auc.png\", topk=TOPK_PLOT)\n",
    "    print(f\"[SAVE] {OUT_DIR/'pi_barplot_auc.png'}\")\n",
    "\n",
    "    # Persist test set for later steps (6–10) to ensure consistency\n",
    "    Path(\"outputs_step2\").mkdir(parents=True, exist_ok=True)\n",
    "    X_test.to_csv(\"outputs_step2/X_test.csv\", index=False)\n",
    "    y_test.to_csv(\"outputs_step2/y_test.csv\", index=False)\n",
    "    print(\"[SAVE] outputs_step2/X_test.csv & outputs_step2/y_test.csv\")\n",
    "\n",
    "    # Save run config\n",
    "    run_cfg = {\n",
    "        \"data_path\": DATA_PATH,\n",
    "        \"model_path\": MODEL_PATH,\n",
    "        \"meta_txt_path\": META_TXT_PATH,\n",
    "        \"n_repeats\": N_REPEATS,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"test_size\": TEST_SIZE,\n",
    "        \"topk_plot\": TOPK_PLOT,\n",
    "        \"exclude_sensitive\": EXCLUDE_SENSITIVE,\n",
    "        \"sensitive_cols\": SENSITIVE_COLS,\n",
    "        \"used_presaved_test\": os.path.exists(\"outputs_step2/X_test.csv\")\n",
    "    }\n",
    "    with open(OUT_DIR / \"config.json\", \"w\") as f:\n",
    "        json.dump(run_cfg, f, indent=2)\n",
    "    print(f\"[SAVE] {OUT_DIR/'config.json'}\")\n",
    "\n",
    "    # Meta sanity check (optional but useful)\n",
    "    if meta_roc is not None:\n",
    "        delta = abs(roc - meta_roc)\n",
    "        if delta < 1e-4:\n",
    "            print(\"[CHECK] ROC-AUC matches Meta.txt ✅\")\n",
    "        else:\n",
    "            print(f\"[WARN] ROC-AUC differs from Meta.txt by {delta:.6f}. Re-check split/processing.\")\n",
    "    if meta_pr is not None:\n",
    "        delta = abs(pr - meta_pr)\n",
    "        if delta < 1e-4:\n",
    "            print(\"[CHECK] PR-AUC matches Meta.txt ✅\")\n",
    "        else:\n",
    "            print(f\"[WARN] PR-AUC differs from Meta.txt by {delta:.6f}. Re-check split/processing.\")\n",
    "\n",
    "    print(\"[DONE] Step 8 completed successfully ✅\")\n",
    "    print(f\"[OUTPUT DIR] {OUT_DIR.resolve()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
