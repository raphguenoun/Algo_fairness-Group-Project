{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa1941c",
   "metadata": {},
   "source": [
    "\n",
    "# Step 7 â€” Local Interpretability with SHAP (on XGBoost)\n",
    "\n",
    "This notebook explains individual predictions of the **Step 2 XGBoost model** using **SHAP**.\n",
    "It loads your fitted model pipeline from `outputs_step2/xgb_step2_model.joblib`, recreates the minimal preprocessing (for safety), and generates global & local SHAP plots.\n",
    "\n",
    "**Outputs** are saved in `outputs_step7/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a64ac90",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0278a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed, install once:\n",
    "# %pip install -q shap\n",
    "\n",
    "import os, sys, platform, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shap\n",
    "import joblib\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "\n",
    "import sklearn, xgboost\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"XGBoost:\", xgboost.__version__)\n",
    "print(\"SHAP:\", shap.__version__)\n",
    "\n",
    "# Output dir\n",
    "os.makedirs(\"outputs_step7\", exist_ok=True)\n",
    "\n",
    "# Enable JS for force plots in-notebook (optional)\n",
    "shap.initjs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a03ebdf",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af954c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths\n",
    "MODEL_PATH_CANDIDATES = [\n",
    "    \"outputs_step2/xgb_step2_model.joblib\",\n",
    "    \"/mnt/data/outputs_step2/xgb_step2_model.joblib\"\n",
    "]\n",
    "DATA_PATH_CANDIDATES = [\n",
    "    \"dataproject2025.csv\",\n",
    "    \"./data/dataproject2025.csv\",\n",
    "    \"/mnt/data/dataproject2025.csv\"\n",
    "]\n",
    "\n",
    "def first_existing(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return paths[0]\n",
    "\n",
    "MODEL_PATH = first_existing(MODEL_PATH_CANDIDATES)\n",
    "DATA_PATH  = first_existing(DATA_PATH_CANDIDATES)\n",
    "\n",
    "# Run parameters\n",
    "USE_SAMPLE = True      # set False for full run\n",
    "SAMPLE_N   = 25000     # rows for quick SHAP\n",
    "BG_N       = 2000      # background size for SHAP explainer\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"MODEL_PATH:\", MODEL_PATH)\n",
    "print(\"DATA_PATH :\", DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eced198",
   "metadata": {},
   "source": [
    "## 2) Helper class for the saved pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2423733",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The Step 2 model was saved as a Pipeline with a custom 'PretrainedPreprocessor' wrapper.\n",
    "# We re-declare it here so joblib can unpickle the model.\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class PretrainedPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, fitted_preprocessor):\n",
    "        self.fitted = fitted_preprocessor\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return self.fitted.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969b0ea",
   "metadata": {},
   "source": [
    "## 3) Load trained model and components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_model = joblib.load(MODEL_PATH)\n",
    "\n",
    "# Unpack components\n",
    "pipeline_steps = final_model.named_steps\n",
    "prep_wrapper   = pipeline_steps[\"prep\"]            # PretrainedPreprocessor\n",
    "prep           = prep_wrapper.fitted               # fitted ColumnTransformer\n",
    "xgb            = pipeline_steps[\"xgb\"]             # fitted XGBClassifier\n",
    "\n",
    "print(\"Loaded model. Booster params:\", xgb.get_params())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a097e6",
   "metadata": {},
   "source": [
    "## 4) Load raw data and minimal cleaning (align with Step 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb061294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read data\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "\n",
    "# Drop known leaky / index columns (from Step 2)\n",
    "LEAKY_COLS = [\n",
    "    \"Predictions\", \"Predicted probabilities\", \"DP\", \"dp\",\n",
    "    \"Unnamed: 0\", \"id\", \"ID\", \"index\"\n",
    "]\n",
    "df = df.drop(columns=[c for c in LEAKY_COLS if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "# Clean infinities\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Ensure Step 2 engineered columns exist if the model expects them\n",
    "def parse_emp_length(val):\n",
    "    if pd.isna(val): return np.nan\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"< 1 year\", \"less than 1 year\", \"<1 year\"}: return 0.5\n",
    "    if s in {\"10+ years\", \"10+ yrs\", \"10+yr\"}: return 10.0\n",
    "    for tok in s.replace(\"+\",\"\").split():\n",
    "        try: return float(int(tok))\n",
    "        except: continue\n",
    "    try: return float(s)\n",
    "    except: return np.nan\n",
    "\n",
    "def parse_loan_duration(val):\n",
    "    if pd.isna(val): return np.nan\n",
    "    if isinstance(val, (int, float)): return float(val)\n",
    "    s = str(val).strip().lower().replace(\"months\",\"\").replace(\"month\",\"\").strip()\n",
    "    try: return float(s)\n",
    "    except: return np.nan\n",
    "\n",
    "if \"emp_length\" in df.columns and \"emp_length_parsed\" not in df.columns:\n",
    "    df[\"emp_length_parsed\"] = df[\"emp_length\"].apply(parse_emp_length)\n",
    "\n",
    "if \"loan duration\" in df.columns and \"loan_duration_parsed\" not in df.columns:\n",
    "    df[\"loan_duration_parsed\"] = df[\"loan duration\"].apply(parse_loan_duration)\n",
    "\n",
    "if \"issue_d\" in df.columns and \"issue_d_ordinal\" not in df.columns:\n",
    "    if df[\"issue_d\"].dtype == object:\n",
    "        d = pd.to_datetime(df[\"issue_d\"], errors=\"coerce\")\n",
    "        df[\"issue_d_ordinal\"] = d.map(lambda x: x.toordinal() if pd.notna(x) else np.nan)\n",
    "\n",
    "print(\"Raw shape:\", df.shape)\n",
    "display(df.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865518aa",
   "metadata": {},
   "source": [
    "## 5) Recover expected feature lists from the fitted preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b11cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ColumnTransformer keeps the column selectors in transformers_\n",
    "num_cols_expected = None\n",
    "cat_cols_expected = None\n",
    "for name, trans, cols in prep.transformers_:\n",
    "    if name == \"num\":\n",
    "        num_cols_expected = list(cols)\n",
    "    elif name == \"cat\":\n",
    "        cat_cols_expected = list(cols)\n",
    "\n",
    "num_cols_expected = num_cols_expected or []\n",
    "cat_cols_expected = cat_cols_expected or []\n",
    "print(\"Expected numeric cols:\", len(num_cols_expected))\n",
    "print(\"Expected categorical cols:\", len(cat_cols_expected))\n",
    "\n",
    "# Warn if any are missing in df\n",
    "missing = [c for c in num_cols_expected + cat_cols_expected if c not in df.columns]\n",
    "if missing:\n",
    "    print(\"WARNING: Missing in current dataframe:\", missing[:20], \"...\" if len(missing)>20 else \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9bd18f",
   "metadata": {},
   "source": [
    "## 6) Build X matrix (optionally sample) and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_order = num_cols_expected + cat_cols_expected\n",
    "X_all = df[feature_order].copy()\n",
    "\n",
    "if USE_SAMPLE and len(X_all) > SAMPLE_N:\n",
    "    X_sample = X_all.sample(SAMPLE_N, random_state=RANDOM_STATE)\n",
    "else:\n",
    "    X_sample = X_all\n",
    "\n",
    "# Transform using fitted preprocessor\n",
    "X_t = prep.transform(X_sample)\n",
    "\n",
    "# Get expanded feature names (after OHE)\n",
    "ohe = None\n",
    "try:\n",
    "    ohe = prep.named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "num_names = list(num_cols_expected)\n",
    "cat_names = list(ohe.get_feature_names_out(cat_cols_expected)) if ohe is not None else []\n",
    "feature_names = num_names + cat_names\n",
    "\n",
    "print(\"Sample shape:\", X_sample.shape, \" -> transformed:\", getattr(X_t, \"shape\", None))\n",
    "print(\"n_features after OHE:\", len(feature_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f84e2d",
   "metadata": {},
   "source": [
    "## 7) Model probabilities on the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f771f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "proba = xgb.predict_proba(X_t)[:, 1]\n",
    "print(\"Proba stats:\", pd.Series(proba).describe().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d78053",
   "metadata": {},
   "source": [
    "## 8) Build SHAP background and explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03754aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Background (reference) distribution\n",
    "N = X_t.shape[0]\n",
    "bg_size = min(BG_N, N)\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "bg_idx = rng.choice(N, size=bg_size, replace=False)\n",
    "X_bg = X_t[bg_idx]\n",
    "\n",
    "# Build TreeExplainer (probability output is intuitive for classification)\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(xgb, data=X_bg, feature_perturbation=\"interventional\", model_output=\"probability\")\n",
    "except Exception as e:\n",
    "    print(\"Primary TreeExplainer failed, falling back to generic Explainer. Error:\", repr(e))\n",
    "    explainer = shap.Explainer(xgb, X_bg, feature_names=feature_names)\n",
    "\n",
    "explainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2592ea",
   "metadata": {},
   "source": [
    "## 9) Compute SHAP values on the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9c3f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer(X_t)  # shap.Explanation\n",
    "print(\"shap_values shape:\", getattr(shap_values.values, \"shape\", None))\n",
    "\n",
    "# Sanity check: additivity (base + sum SHAP â‰ˆ proba)\n",
    "approx = shap_values.base_values + shap_values.values.sum(axis=1)\n",
    "corr = np.corrcoef(approx, proba)[0,1]\n",
    "mae  = np.mean(np.abs(approx - proba))\n",
    "print(f\"Additivity check â€” corr: {corr:.6f}, MAE: {mae:.6e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8941b9f",
   "metadata": {},
   "source": [
    "## 10) Global SHAP plots (beeswarm & bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b73931",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "shap.plots.beeswarm(shap_values, max_display=30, show=False)\n",
    "plt.title(\"SHAP Beeswarm â€” Top 30\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs_step7/shap_beeswarm_top30.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "shap.plots.bar(shap_values, max_display=30, show=False)\n",
    "plt.title(\"Mean |SHAP| â€” Top 30\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs_step7/shap_bar_top30.png\", dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf3600c",
   "metadata": {},
   "source": [
    "## 11) Dependence plots (top features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d947e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "# Rank by mean |SHAP|\n",
    "mean_abs = np.abs(shap_values.values).mean(axis=0)\n",
    "topk_idx = np.argsort(mean_abs)[::-1][:10]\n",
    "\n",
    "for idx in topk_idx:\n",
    "    try:\n",
    "        shap.plots.scatter(shap_values[:, idx], color=shap_values, show=False)\n",
    "        plt.title(f\"SHAP dependence â€” {feature_names[idx]}\")\n",
    "        plt.tight_layout()\n",
    "        fname = f\"outputs_step7/dep_{idx}_{feature_names[idx][:40].replace('/','_')}.png\"\n",
    "        plt.savefig(fname, dpi=200)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Dependence plot failed for\", feature_names[idx], \"->\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6aa659",
   "metadata": {},
   "source": [
    "## 12) Local explanations â€” Waterfall plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adbf891",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pick a few representative cases: highest/lowest/borderline + randoms\n",
    "ix_max = int(np.argmax(proba))\n",
    "ix_min = int(np.argmin(proba))\n",
    "# pick one close to 0.5 if possible\n",
    "dif = np.abs(proba - 0.5)\n",
    "ix_mid = int(np.argmin(dif))\n",
    "\n",
    "candidates = sorted(set([ix_max, ix_min, ix_mid]))\n",
    "# add a couple of random indices\n",
    "rng = np.random.default_rng(123)\n",
    "rand_extra = rng.choice(X_t.shape[0], size=min(2, X_t.shape[0]), replace=False).tolist()\n",
    "for ix in rand_extra:\n",
    "    if ix not in candidates:\n",
    "        candidates.append(int(ix))\n",
    "\n",
    "print(\"Waterfall indices:\", candidates)\n",
    "\n",
    "for ix in candidates:\n",
    "    try:\n",
    "        shap.plots.waterfall(shap_values[ix], max_display=20, show=False)\n",
    "        plt.title(f\"Waterfall â€” idx {ix}, proba={proba[ix]:.3f}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"outputs_step7/waterfall_idx{ix}.png\", dpi=200)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Waterfall failed for idx\", ix, \"->\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319eef94",
   "metadata": {},
   "source": [
    "## 13) Save global SHAP CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (a) mean |SHAP| per expanded feature\n",
    "import pandas as pd\n",
    "sv_df = pd.DataFrame(shap_values.values, columns=feature_names)\n",
    "mean_abs = sv_df.abs().mean().sort_values(ascending=False)\n",
    "mean_abs.to_csv(\"outputs_step7/global_mean_abs_shap_expanded.csv\", header=[\"mean_abs_shap\"])\n",
    "\n",
    "print(\"Saved: outputs_step7/global_mean_abs_shap_expanded.csv\")\n",
    "\n",
    "# (b) Optional: aggregate OHE back to original categorical names (sum of |SHAP| across levels)\n",
    "agg = {}\n",
    "for f in feature_names:\n",
    "    # heuristic: OneHotEncoder names look like \"feature_category\"\n",
    "    base = f.split(\"_\", 1)[0] if \"_\" in f else f\n",
    "    agg.setdefault(base, 0.0)\n",
    "    agg[base] += mean_abs.get(f, 0.0)\n",
    "\n",
    "agg = pd.Series(agg).sort_values(ascending=False)\n",
    "agg.to_csv(\"outputs_step7/global_mean_abs_shap_grouped.csv\", header=[\"grouped_mean_abs_shap\"])\n",
    "print(\"Saved: outputs_step7/global_mean_abs_shap_grouped.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2eb084",
   "metadata": {},
   "source": [
    "## 14) Notes and acceptance checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec0baf",
   "metadata": {},
   "source": [
    "\n",
    "- **Additivity sanity**: `(base + sum SHAP)` should closely match predicted probability (see printed corr/MAE).\n",
    "- **Top drivers**: Review `shap_bar_top30.png` and `shap_beeswarm_top30.png` for global importance and direction.\n",
    "- **Local decisions**: Inspect `waterfall_*.png` to see per-feature pushes toward â†‘/â†“ default probability.\n",
    "- **Grouping**: Use `global_mean_abs_shap_grouped.csv` as a semantic ranking (categoricals aggregated across levels).\n",
    "- **Performance**: Increase `SAMPLE_N` or set `USE_SAMPLE=False` for a full run. Keep `BG_N` ~1â€“2k for speed/stability.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
