{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad50461",
   "metadata": {},
   "source": [
    "\n",
    "# Step 2 — Black‑Box Model (XGBoost) for Default Prediction\n",
    "\n",
    "This notebook trains an XGBoost classifier to forecast **default** (binary `target`) for the course project **Interpretability, Stability, and Algorithmic Fairness (Fall 2025)**.\n",
    "\n",
    "**What this notebook does**\n",
    "- Loads the project dataset\n",
    "- Cleans & preprocesses features (numeric + categorical)\n",
    "- Trains an XGBoost model with early stopping\n",
    "- Evaluates with ROC‑AUC, PR‑AUC, F1, confusion matrix\n",
    "- Saves predictions and the fitted model for later steps\n",
    "\n",
    "> Note: Columns that could leak information from Step 1 (e.g., `'Predictions'`, `'Predicted probabilities'`) are **dropped** on load.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f206e70d",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa225ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed, install dependencies (uncomment as necessary)\n",
    "# %pip install -q xgboost scikit-learn pandas numpy matplotlib joblib\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, precision_recall_curve,\n",
    "    roc_curve, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcee547",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1425e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Paths ---\n",
    "# Prefer a local file first; fall back to /mnt/data if running in a hosted environment.\n",
    "CANDIDATE_PATHS = [\n",
    "    \"dataproject2025.csv\",\n",
    "    \"./data/dataproject2025.csv\",\n",
    "    \"/mnt/data/dataproject2025.csv\"\n",
    "]\n",
    "DATA_PATH = next((p for p in CANDIDATE_PATHS if os.path.exists(p)), CANDIDATE_PATHS[-1])\n",
    "\n",
    "# --- Basic run params ---\n",
    "TEST_SIZE = 0.20            # 80/20 stratified split\n",
    "USE_SAMPLE = False          # set True for quick tests (e.g., 50_000 rows)\n",
    "SAMPLE_N = 50_000\n",
    "\n",
    "# Whether to EXCLUDE sensitive attributes from features (you can toggle later for Step 9 fairness analysis)\n",
    "EXCLUDE_SENSITIVE = False\n",
    "SENSITIVE_COLS = [\n",
    "    \"Pct_afro_american\"    # protected attribute proxy provided in the dataset\n",
    "]\n",
    "\n",
    "print(f\"Using DATA_PATH = {DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de2bf1",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae8264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read CSV (large-friendly). We'll downcast floats to float32 to reduce memory.\n",
    "read_kwargs = dict(low_memory=False)\n",
    "df = pd.read_csv(DATA_PATH, **read_kwargs)\n",
    "\n",
    "if USE_SAMPLE and len(df) > SAMPLE_N:\n",
    "    df = df.sample(SAMPLE_N, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "print(df.shape)\n",
    "display(df.head(3))\n",
    "print(\"\\nColumns:\", list(df.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1405286",
   "metadata": {},
   "source": [
    "## 3. Basic cleaning & target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f82a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop known leak / index columns if present\n",
    "LEAKY_COLS = [\n",
    "    \"Predictions\", \"Predicted probabilities\", \"DP\", \"dp\", \n",
    "    \"Unnamed: 0\", \"id\", \"ID\", \"index\"\n",
    "]\n",
    "present_leaky = [c for c in LEAKY_COLS if c in df.columns]\n",
    "df = df.drop(columns=present_leaky, errors=\"ignore\")\n",
    "\n",
    "# Target\n",
    "assert \"target\" in df.columns, \"Expected 'target' column not found.\"\n",
    "df[\"target\"] = df[\"target\"].astype(int)\n",
    "\n",
    "# Optional: exclude sensitive columns for Step 2 (toggle above)\n",
    "if EXCLUDE_SENSITIVE:\n",
    "    drop_sens = [c for c in SENSITIVE_COLS if c in df.columns]\n",
    "    if drop_sens:\n",
    "        df = df.drop(columns=drop_sens)\n",
    "\n",
    "print(\"After drops:\", df.shape)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fd30f",
   "metadata": {},
   "source": [
    "## 4. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_emp_length(val):\n",
    "    '''\n",
    "    Convert employment length strings to numeric years.\n",
    "    Examples: '10+ years'->10, '< 1 year'->0.5, '3 years'->3, NaN->np.nan\n",
    "    '''\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"< 1 year\", \"less than 1 year\", \"<1 year\"}:\n",
    "        return 0.5\n",
    "    if s in {\"10+ years\", \"10+ yrs\", \"10+yr\"}:\n",
    "        return 10.0\n",
    "    # Extract any leading integer\n",
    "    for tok in s.replace(\"+\",\"\").split():\n",
    "        try:\n",
    "            return float(int(tok))\n",
    "        except:\n",
    "            continue\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def parse_loan_duration(val):\n",
    "    '''\n",
    "    Convert '36 months' -> 36, '60 months' -> 60, already numeric -> itself.\n",
    "    '''\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    if isinstance(val, (int, float)):\n",
    "        return float(val)\n",
    "    s = str(val).strip().lower().replace(\"months\",\"\").replace(\"month\",\"\").strip()\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Apply parsers if columns exist\n",
    "if \"emp_length\" in df.columns:\n",
    "    df[\"emp_length_parsed\"] = df[\"emp_length\"].apply(parse_emp_length)\n",
    "if \"loan duration\" in df.columns:\n",
    "    df[\"loan_duration_parsed\"] = df[\"loan duration\"].apply(parse_loan_duration)\n",
    "\n",
    "# If 'issue_d' is a date-like string or year, keep numeric\n",
    "if \"issue_d\" in df.columns:\n",
    "    # If looks numeric (e.g., 2013.0), keep as float; else parse to ordinal\n",
    "    if df[\"issue_d\"].dtype == object:\n",
    "        try:\n",
    "            d = pd.to_datetime(df[\"issue_d\"], errors=\"coerce\")\n",
    "            df[\"issue_d_ordinal\"] = d.map(lambda x: x.toordinal() if pd.notna(x) else np.nan)\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        # leave numeric year/month values as-is\n",
    "        pass\n",
    "\n",
    "# Identify feature columns\n",
    "target_col = \"target\"\n",
    "all_features = [c for c in df.columns if c != target_col]\n",
    "\n",
    "# Separate numeric vs categorical by dtype (object -> categorical). We'll skip free-text high-cardinality fields.\n",
    "high_cardinality_text = {\"emp_title\"}  # can revisit with hashing later\n",
    "features = [c for c in all_features if c not in high_cardinality_text]\n",
    "\n",
    "numeric_cols = [c for c in features if pd.api.types.is_numeric_dtype(df[c])]\n",
    "categorical_cols = [c for c in features if c not in numeric_cols]\n",
    "\n",
    "print(\"Numeric:\", len(numeric_cols), \"Categorical:\", len(categorical_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac102d7",
   "metadata": {},
   "source": [
    "## 5. Train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e972120",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df[numeric_cols + categorical_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "# Ensure numeric types are float32 to reduce memory footprint\n",
    "for c in numeric_cols:\n",
    "    X[c] = pd.to_numeric(X[c], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \" Test:\", X_test.shape)\n",
    "print(\"Default rate (train):\", y_train.mean().round(4), \" (test):\", y_test.mean().round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4757aa",
   "metadata": {},
   "source": [
    "## 6. Model: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc87a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess\n",
    "numeric_transformer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols),\n",
    "    ],\n",
    "    sparse_threshold=0.3  # keep sparse matrices when many categories\n",
    ")\n",
    "\n",
    "# Class imbalance handling via scale_pos_weight = (neg/pos)\n",
    "pos = y_train.sum()\n",
    "neg = len(y_train) - pos\n",
    "scale_pos_weight = float(neg / max(pos, 1))\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=1.0,\n",
    "    min_child_weight=1.0,\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Early stopping with a validation split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.15, stratify=y_train, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"xgb\", xgb)\n",
    "])\n",
    "\n",
    "# Fit with early stopping (pass eval_set to underlying xgb via fit_params)\n",
    "# We need to transform X_val with the preprocessor to feed to xgb for eval_set\n",
    "prep = preprocess.fit(X_tr, y_tr)\n",
    "X_tr_t = prep.transform(X_tr)\n",
    "X_val_t = prep.transform(X_val)\n",
    "\n",
    "xgb.fit(\n",
    "    X_tr_t, y_tr,\n",
    "    eval_set=[(X_val_t, y_val)],\n",
    "    verbose=False,\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# Wrap back into a single pipeline for convenience (prep is already fitted; xgb is fitted).\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class PretrainedPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, fitted_preprocessor):\n",
    "        self.fitted = fitted_preprocessor\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return self.fitted.transform(X)\n",
    "\n",
    "final_model = Pipeline(steps=[\n",
    "    (\"prep\", PretrainedPreprocessor(prep)),\n",
    "    (\"xgb\", xgb)\n",
    "])\n",
    "\n",
    "try:\n",
    "    best_ntree = xgb.get_booster().best_ntree_limit\n",
    "except Exception:\n",
    "    best_ntree = \"n/a\"\n",
    "print(\"Best n_estimators (limit):\", best_ntree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65defc",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16470491",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict\n",
    "X_test_t = prep.transform(X_test)\n",
    "proba = xgb.predict_proba(X_test_t)[:, 1]\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "roc_auc = roc_auc_score(y_test, proba)\n",
    "pr_auc = average_precision_score(y_test, proba)\n",
    "print(f\"ROC-AUC: {roc_auc:.4f} | PR-AUC: {pr_auc:.4f}\\n\")\n",
    "\n",
    "print(classification_report(y_test, pred, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC-AUC={roc_auc:.3f}\")\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve — XGBoost (Step 2)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# PR Curve\n",
    "prec, rec, _ = precision_recall_curve(y_test, proba)\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, label=f\"PR-AUC={pr_auc:.3f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve — XGBoost (Step 2)\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0caca2",
   "metadata": {},
   "source": [
    "## 8. Feature importance (gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Map feature names through the fitted preprocessor (OHE expands categories)\n",
    "ohe = prep.named_transformers_[\"cat\"].named_steps[\"ohe\"] if len(categorical_cols) else None\n",
    "num_feature_names = numeric_cols\n",
    "cat_feature_names = list(ohe.get_feature_names_out(categorical_cols)) if ohe is not None else []\n",
    "feature_names = list(num_feature_names) + cat_feature_names\n",
    "\n",
    "booster = xgb.get_booster()\n",
    "# XGBoost importance is keyed by f0, f1, ...\n",
    "score_map = booster.get_score(importance_type=\"gain\")\n",
    "\n",
    "# Convert to a sorted DataFrame\n",
    "imp = pd.DataFrame(\n",
    "    [(int(k[1:]), v) for k,v in score_map.items()],\n",
    "    columns=[\"index\",\"gain\"]\n",
    ").sort_values(\"gain\", ascending=False)\n",
    "\n",
    "imp[\"feature\"] = imp[\"index\"].map(lambda i: feature_names[i] if i < len(feature_names) else f\"f{i}\")\n",
    "topk = imp.head(30)[[\"feature\",\"gain\"]]\n",
    "display(topk)\n",
    "\n",
    "plt.figure()\n",
    "plt.barh(topk[\"feature\"][::-1], topk[\"gain\"][::-1])\n",
    "plt.xlabel(\"Gain importance\")\n",
    "plt.title(\"Top features — XGBoost (gain)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e1e41e",
   "metadata": {},
   "source": [
    "## 9. Save artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c26a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_dir = \"outputs_step2\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(out_dir, \"xgb_step2_model.joblib\")\n",
    "preds_path = os.path.join(out_dir, \"xgb_step2_test_predictions.csv\")\n",
    "meta_path  = os.path.join(out_dir, \"xgb_step2_meta.txt\")\n",
    "\n",
    "# Save model and predictions\n",
    "joblib.dump(final_model, model_path)\n",
    "\n",
    "preds_df = pd.DataFrame({\n",
    "    \"y_true\": y_test.values,\n",
    "    \"y_proba\": proba,\n",
    "    \"y_pred\": pred\n",
    "})\n",
    "preds_df.to_csv(preds_path, index=False)\n",
    "\n",
    "with open(meta_path, \"w\") as f:\n",
    "    f.write(f\"Saved: {datetime.now().isoformat()}\\n\")\n",
    "    f.write(f\"Data path: {DATA_PATH}\\n\")\n",
    "    f.write(f\"Train shape: {X_train.shape} | Test shape: {X_test.shape}\\n\")\n",
    "    f.write(f\"ROC-AUC: {roc_auc:.6f} | PR-AUC: {pr_auc:.6f}\\n\")\n",
    "    f.write(f\"Scale_pos_weight: {scale_pos_weight}\\n\")\n",
    "    f.write(f\"Excluded sensitive cols: {EXCLUDE_SENSITIVE}\\n\")\n",
    "\n",
    "print(\"Artifacts saved to:\", out_dir)\n",
    "print(\" -\", model_path)\n",
    "print(\" -\", preds_path)\n",
    "print(\" -\", meta_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acd63d",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Notes for next steps\n",
    "- **Step 3 (Stability):** keep the `outputs_step2/xgb_step2_model.joblib` and `xgb_step2_test_predictions.csv` for temporal or cohort‑based stability checks.\n",
    "- **Steps 4–8 (Interpretability):** you can reuse `final_model` to compute SHAP, LIME, PDP, ICE, and permutation importance.\n",
    "- **Step 9–10 (Fairness):** toggle `EXCLUDE_SENSITIVE` to compare performance and fairness with/without protected attributes.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
